{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert generated (returns and cond nums) data to usable format to pass to env\n",
    "- check action bound: are actions in [-1,1]?\n",
    "- should we generate 1 long seqence or multiple? how does it effect convergence? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running from jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for compatible with python 3\n",
    "from __future__ import print_function\n",
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "import numpy as np\n",
    "from utils.data import read_stock_history, index_to_date, date_to_index, normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ddpg.actor import ActorNetwork\n",
    "from model.ddpg.critic import CriticNetwork\n",
    "from model.ddpg.ddpg import DDPG\n",
    "from model.ddpg.ornstein_uhlenbeck import OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from stock_trading import StockActor, StockCritic, obs_normalizer, get_model_path, get_result_path, \\\n",
    "                          test_model, get_variable_scope, test_model_multiple, convert_R_output\n",
    "    \n",
    "from model.supervised.lstm import StockLSTM\n",
    "from model.supervised.cnn import StockCNN\n",
    "\n",
    "from environment.portfolio import PortfolioEnv, MultiActionPortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters calculated in R\n",
    "filepath = '/Users/zachariemartin/Desktop/School/Projects/summer2019/2/sum19/'\n",
    "\n",
    "alpha, beta, omega, Q_bar = convert_R_output(coef_path = filepath + 'coef.csv',\\\n",
    "                                                   Q_bar_path = filepath + 'Q_bar.csv', num_assets=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common settings\n",
    "batch_size = 64\n",
    "action_bound = 1.\n",
    "tau = 1e-3\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "window_length_lst = [3]\n",
    "predictor_type_lst = ['cnn']\n",
    "use_batch_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv = PortfolioEnv(target_history, target_stocks)\\n\\nfor window_length in window_length_lst:\\n    for predictor_type in predictor_type_lst:\\n        name = 'DDPG_window_{}_predictor_{}'.format(window_length, predictor_type)\\n        model_names.append(name)\\n        tf.reset_default_graph()\\n        sess = tf.Session()\\n        tflearn.config.init_training_mode()\\n        action_dim = [nb_classes]\\n        state_dim = [nb_classes, window_length]\\n        variable_scope = get_variable_scope(window_length, predictor_type, use_batch_norm)\\n        with tf.variable_scope(variable_scope):\\n            actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, predictor_type, \\n                               use_batch_norm)\\n            critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\\n                                 learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(), \\n                                 predictor_type=predictor_type, use_batch_norm=use_batch_norm)\\n            actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\\n\\n            model_save_path = get_model_path(window_length, predictor_type, use_batch_norm)\\n            summary_path = get_result_path(window_length, predictor_type, use_batch_norm)\\n\\n            ddpg_model = DDPG(env, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\\n                              config_file='config/stock.json', model_save_path=model_save_path,\\n                              summary_path=summary_path)\\n            ddpg_model.initialize(load_weights=False, verbose=False)\\n            models.append(ddpg_model)\\n            \\nddpg_model.train()\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go from passing history in to env to generating within env\n",
    "\n",
    "'''\n",
    "env = PortfolioEnv(target_history, target_stocks)\n",
    "\n",
    "for window_length in window_length_lst:\n",
    "    for predictor_type in predictor_type_lst:\n",
    "        name = 'DDPG_window_{}_predictor_{}'.format(window_length, predictor_type)\n",
    "        model_names.append(name)\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        tflearn.config.init_training_mode()\n",
    "        action_dim = [nb_classes]\n",
    "        state_dim = [nb_classes, window_length]\n",
    "        variable_scope = get_variable_scope(window_length, predictor_type, use_batch_norm)\n",
    "        with tf.variable_scope(variable_scope):\n",
    "            actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, predictor_type, \n",
    "                               use_batch_norm)\n",
    "            critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\n",
    "                                 learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(), \n",
    "                                 predictor_type=predictor_type, use_batch_norm=use_batch_norm)\n",
    "            actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "            model_save_path = get_model_path(window_length, predictor_type, use_batch_norm)\n",
    "            summary_path = get_result_path(window_length, predictor_type, use_batch_norm)\n",
    "\n",
    "            ddpg_model = DDPG(env, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\n",
    "                              config_file='config/stock.json', model_save_path=model_save_path,\n",
    "                              summary_path=summary_path)\n",
    "            ddpg_model.initialize(load_weights=False, verbose=False)\n",
    "            models.append(ddpg_model)\n",
    "            \n",
    "ddpg_model.train()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generated\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# # param dict\n",
    "# parameters = {}\n",
    "\n",
    "# # 10 assets\n",
    "num_assets = 10\n",
    "num_rows_per_asset = 5\n",
    "\n",
    "# # calculated in R\n",
    "# a = 0.007073296\n",
    "# b = 0.658588119\n",
    "\n",
    "# # import from R\n",
    "coef = pd.read_csv(filepath + 'coef.csv')\n",
    "\n",
    "# # mu, ar1, omega, alpha, beta 0,1,2,3,4 for each asset - 5 values for each asset so 500 total values\n",
    "# parameters['alpha'] = np.array([coef.loc[i,'x'] for i in range(3,(num_assets*num_rows_per_asset),5)])\n",
    "# parameters['beta'] = np.array([coef.loc[i,'x'] for i in range(4,(num_assets*num_rows_per_asset),5)])\n",
    "# parameters['omega'] = np.array([coef.loc[i,'x'] for i in range(2,(num_assets*num_rows_per_asset),5)])\n",
    "# parameters['Q_bar'] = pd.read_csv('Q_bar.csv').drop('Unnamed: 0',axis=1).to_numpy()\n",
    "# parameters['H_init'] = pd.read_csv('H_init.csv').drop('Unnamed: 0',axis=1).to_numpy()\n",
    "# parameters['Q'] = pd.read_csv('Q_init.csv').drop('Unnamed: 0',axis=1).to_numpy()\n",
    "# parameters['T'] = 1000\n",
    "# parameters['small_scalar'] = 1e-5\n",
    "# parameters['num_assets'] = 10\n",
    "\n",
    "parameters = {\n",
    "                'alpha' : np.array([coef.loc[i,'x'] for i in range(3,(num_assets*num_rows_per_asset),5)]),\n",
    "                'beta' : np.array([coef.loc[i,'x'] for i in range(4,(num_assets*num_rows_per_asset),5)]),\n",
    "                'omega' : np.array([coef.loc[i,'x'] for i in range(2,(num_assets*num_rows_per_asset),5)]),\n",
    "                'Q_bar' : pd.read_csv(filepath + 'Q_bar.csv').drop('Unnamed: 0',axis=1).to_numpy(),\n",
    "                'H_init' : pd.read_csv(filepath + 'H_init.csv').drop('Unnamed: 0',axis=1).to_numpy(),\n",
    "                'Q' : pd.read_csv(filepath + 'Q_init.csv').drop('Unnamed: 0',axis=1).to_numpy(),\n",
    "                'T' : 1000,\n",
    "                'small_scalar' : 1e-5,\n",
    "                'num_assets' : 10,\n",
    "                # calculated in R - parameters for Q process\n",
    "                'a' : 0.007073296,\n",
    "                'b' : 0.658588119,\n",
    "                'mean' : 100\n",
    "}\n",
    "\n",
    "env = PortfolioEnv(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = parameters['num_assets'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "Build model from scratch\n"
     ]
    }
   ],
   "source": [
    "for window_length in window_length_lst:\n",
    "    for predictor_type in predictor_type_lst:\n",
    "        name = 'DDPG_window_{}_predictor_{}'.format(window_length, predictor_type)\n",
    "        model_names.append(name)\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        tflearn.config.init_training_mode()\n",
    "        action_dim = [nb_classes]\n",
    "        state_dim = [nb_classes, window_length]\n",
    "        variable_scope = get_variable_scope(window_length, predictor_type, use_batch_norm)\n",
    "        with tf.variable_scope(variable_scope):\n",
    "            actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, predictor_type, \n",
    "                               use_batch_norm)\n",
    "            critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\n",
    "                                 learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(), \n",
    "                                 predictor_type=predictor_type, use_batch_norm=use_batch_norm)\n",
    "            actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "            model_save_path = get_model_path(window_length, predictor_type, use_batch_norm)\n",
    "            summary_path = get_result_path(window_length, predictor_type, use_batch_norm)\n",
    "\n",
    "            ddpg_model = DDPG(env, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\n",
    "                              config_file='config/stock.json', model_save_path=model_save_path,\n",
    "                              summary_path=summary_path)\n",
    "            ddpg_model.initialize(load_weights=False, verbose=True)\n",
    "            models.append(ddpg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "Number of episodes:  500\n",
      "Episode: 0 Replay Buffer 0\n",
      "resetting\n",
      "Episode: 0, Reward: 0.00014532, Qmax: 0.00028298\n",
      "Episode: 1 Replay Buffer 731\n",
      "resetting\n",
      "Episode: 1, Reward: 0.00036220, Qmax: 0.00030536\n",
      "Episode: 2 Replay Buffer 1462\n",
      "resetting\n",
      "Episode: 2, Reward: 0.00008384, Qmax: 0.00030220\n",
      "Episode: 3 Replay Buffer 2193\n",
      "resetting\n",
      "Episode: 3, Reward: 0.00086087, Qmax: 0.00030040\n",
      "Episode: 4 Replay Buffer 2924\n",
      "resetting\n",
      "Episode: 4, Reward: 0.00078913, Qmax: 0.00029815\n",
      "Episode: 5 Replay Buffer 3655\n",
      "resetting\n",
      "Episode: 5, Reward: 0.00130539, Qmax: 0.00029476\n",
      "Episode: 6 Replay Buffer 4386\n",
      "resetting\n",
      "Episode: 6, Reward: 0.00079638, Qmax: 0.00029412\n",
      "Episode: 7 Replay Buffer 5117\n",
      "resetting\n",
      "Episode: 7, Reward: 0.00052881, Qmax: 0.00029290\n",
      "Episode: 8 Replay Buffer 5848\n",
      "resetting\n",
      "Episode: 8, Reward: 0.00048126, Qmax: 0.00029188\n",
      "Episode: 9 Replay Buffer 6579\n",
      "resetting\n",
      "Episode: 9, Reward: 0.00002979, Qmax: 0.00028807\n",
      "Episode: 10 Replay Buffer 7310\n",
      "resetting\n",
      "Episode: 10, Reward: -0.00001107, Qmax: 0.00028465\n",
      "Episode: 11 Replay Buffer 8041\n",
      "resetting\n",
      "Episode: 11, Reward: 0.00034454, Qmax: 0.00028267\n",
      "Episode: 12 Replay Buffer 8772\n",
      "resetting\n",
      "Episode: 12, Reward: -0.00005091, Qmax: 0.00027955\n",
      "Episode: 13 Replay Buffer 9503\n",
      "resetting\n",
      "Episode: 13, Reward: 0.00007914, Qmax: 0.00028102\n",
      "Episode: 14 Replay Buffer 10234\n",
      "resetting\n",
      "Episode: 14, Reward: 0.00033447, Qmax: 0.00027819\n",
      "Episode: 15 Replay Buffer 10965\n",
      "resetting\n",
      "Episode: 15, Reward: 0.00078647, Qmax: 0.00027500\n",
      "Episode: 16 Replay Buffer 11696\n",
      "resetting\n",
      "Episode: 16, Reward: 0.00059515, Qmax: 0.00027295\n",
      "Episode: 17 Replay Buffer 12427\n",
      "resetting\n",
      "Episode: 17, Reward: 0.00108176, Qmax: 0.00027299\n",
      "Episode: 18 Replay Buffer 13158\n",
      "resetting\n",
      "Episode: 18, Reward: 0.00128943, Qmax: 0.00027115\n",
      "Episode: 19 Replay Buffer 13889\n",
      "resetting\n",
      "Episode: 19, Reward: 0.00035301, Qmax: 0.00026642\n",
      "Episode: 20 Replay Buffer 14620\n",
      "resetting\n",
      "Episode: 20, Reward: 0.00007523, Qmax: 0.00026857\n",
      "Episode: 21 Replay Buffer 15351\n",
      "resetting\n",
      "Episode: 21, Reward: 0.00061335, Qmax: 0.00026284\n",
      "Episode: 22 Replay Buffer 16082\n",
      "resetting\n",
      "Episode: 22, Reward: 0.00067613, Qmax: 0.00026364\n",
      "Episode: 23 Replay Buffer 16813\n",
      "resetting\n",
      "Episode: 23, Reward: 0.00054601, Qmax: 0.00026304\n",
      "Episode: 24 Replay Buffer 17544\n",
      "resetting\n",
      "Episode: 24, Reward: 0.00057386, Qmax: 0.00026163\n",
      "Episode: 25 Replay Buffer 18275\n",
      "resetting\n",
      "Episode: 25, Reward: 0.00038439, Qmax: 0.00026194\n",
      "Episode: 26 Replay Buffer 19006\n",
      "resetting\n",
      "Episode: 26, Reward: 0.00032393, Qmax: 0.00026161\n",
      "Episode: 27 Replay Buffer 19737\n",
      "resetting\n",
      "Episode: 27, Reward: 0.00015552, Qmax: 0.00026295\n",
      "Episode: 28 Replay Buffer 20468\n",
      "resetting\n",
      "Episode: 28, Reward: 0.00045924, Qmax: 0.00025962\n",
      "Episode: 29 Replay Buffer 21199\n",
      "resetting\n",
      "Episode: 29, Reward: 0.00021222, Qmax: 0.00025754\n",
      "Episode: 30 Replay Buffer 21930\n",
      "resetting\n",
      "Episode: 30, Reward: -0.00003109, Qmax: 0.00025455\n",
      "Episode: 31 Replay Buffer 22661\n",
      "resetting\n",
      "Episode: 31, Reward: 0.00072689, Qmax: 0.00025130\n",
      "Episode: 32 Replay Buffer 23392\n",
      "resetting\n",
      "Episode: 32, Reward: -0.00001280, Qmax: 0.00025288\n",
      "Episode: 33 Replay Buffer 24123\n",
      "resetting\n",
      "Episode: 33, Reward: 0.00023815, Qmax: 0.00024869\n",
      "Episode: 34 Replay Buffer 24854\n",
      "resetting\n",
      "Episode: 34, Reward: 0.00013862, Qmax: 0.00024826\n",
      "Episode: 35 Replay Buffer 25585\n",
      "resetting\n",
      "Episode: 35, Reward: 0.00014318, Qmax: 0.00024574\n",
      "Episode: 36 Replay Buffer 26316\n",
      "resetting\n",
      "Episode: 36, Reward: 0.00034074, Qmax: 0.00024250\n",
      "Episode: 37 Replay Buffer 27047\n",
      "resetting\n",
      "Episode: 37, Reward: 0.00040966, Qmax: 0.00024446\n",
      "Episode: 38 Replay Buffer 27778\n",
      "resetting\n",
      "Episode: 38, Reward: 0.00048915, Qmax: 0.00024413\n",
      "Episode: 39 Replay Buffer 28509\n",
      "resetting\n",
      "Episode: 39, Reward: 0.00017064, Qmax: 0.00024444\n",
      "Episode: 40 Replay Buffer 29240\n",
      "resetting\n",
      "Episode: 40, Reward: -0.00011310, Qmax: 0.00023908\n",
      "Episode: 41 Replay Buffer 29971\n",
      "resetting\n",
      "Episode: 41, Reward: 0.00040635, Qmax: 0.00023880\n",
      "Episode: 42 Replay Buffer 30702\n",
      "resetting\n",
      "Episode: 42, Reward: 0.00083028, Qmax: 0.00024068\n",
      "Episode: 43 Replay Buffer 31433\n",
      "resetting\n",
      "Episode: 43, Reward: 0.00049203, Qmax: 0.00023649\n",
      "Episode: 44 Replay Buffer 32164\n",
      "resetting\n",
      "Episode: 44, Reward: 0.00039384, Qmax: 0.00023474\n",
      "Episode: 45 Replay Buffer 32895\n",
      "resetting\n",
      "Episode: 45, Reward: 0.00097387, Qmax: 0.00022772\n",
      "Episode: 46 Replay Buffer 33626\n",
      "resetting\n",
      "Episode: 46, Reward: 0.00077815, Qmax: 0.00022796\n",
      "Episode: 47 Replay Buffer 34357\n",
      "resetting\n",
      "Episode: 47, Reward: 0.00035468, Qmax: 0.00022854\n",
      "Episode: 48 Replay Buffer 35088\n",
      "resetting\n",
      "Episode: 48, Reward: 0.00010435, Qmax: 0.00022180\n",
      "Episode: 49 Replay Buffer 35819\n",
      "resetting\n",
      "Episode: 49, Reward: 0.00030331, Qmax: 0.00022014\n",
      "Episode: 50 Replay Buffer 36550\n",
      "resetting\n",
      "Episode: 50, Reward: 0.00002679, Qmax: 0.00021922\n",
      "Episode: 51 Replay Buffer 37281\n",
      "resetting\n",
      "Episode: 51, Reward: 0.00076599, Qmax: 0.00021609\n",
      "Episode: 52 Replay Buffer 38012\n",
      "resetting\n",
      "Episode: 52, Reward: 0.00014021, Qmax: 0.00021966\n",
      "Episode: 53 Replay Buffer 38743\n",
      "resetting\n",
      "Episode: 53, Reward: 0.00018239, Qmax: 0.00021067\n",
      "Episode: 54 Replay Buffer 39474\n",
      "resetting\n",
      "Episode: 54, Reward: 0.00036068, Qmax: 0.00021178\n",
      "Episode: 55 Replay Buffer 40205\n",
      "resetting\n",
      "Episode: 55, Reward: 0.00044869, Qmax: 0.00020770\n",
      "Episode: 56 Replay Buffer 40936\n",
      "resetting\n",
      "Episode: 56, Reward: 0.00087810, Qmax: 0.00020836\n",
      "Episode: 57 Replay Buffer 41667\n",
      "resetting\n",
      "Episode: 57, Reward: 0.00080779, Qmax: 0.00020895\n",
      "Episode: 58 Replay Buffer 42398\n",
      "resetting\n",
      "Episode: 58, Reward: 0.00048108, Qmax: 0.00020278\n",
      "Episode: 59 Replay Buffer 43129\n",
      "resetting\n",
      "Episode: 59, Reward: -0.00031865, Qmax: 0.00020019\n",
      "Episode: 60 Replay Buffer 43860\n",
      "resetting\n",
      "Episode: 60, Reward: 0.00027579, Qmax: 0.00020396\n",
      "Episode: 61 Replay Buffer 44591\n",
      "resetting\n",
      "Episode: 61, Reward: 0.00082375, Qmax: 0.00020297\n",
      "Episode: 62 Replay Buffer 45322\n",
      "resetting\n",
      "Episode: 62, Reward: 0.00057819, Qmax: 0.00020115\n",
      "Episode: 63 Replay Buffer 46053\n",
      "resetting\n",
      "Episode: 63, Reward: 0.00004124, Qmax: 0.00019871\n",
      "Episode: 64 Replay Buffer 46784\n",
      "resetting\n",
      "Episode: 64, Reward: 0.00045919, Qmax: 0.00019330\n",
      "Episode: 65 Replay Buffer 47515\n",
      "resetting\n",
      "Episode: 65, Reward: 0.00095214, Qmax: 0.00019948\n",
      "Episode: 66 Replay Buffer 48246\n",
      "resetting\n",
      "Episode: 66, Reward: 0.00069760, Qmax: 0.00019406\n",
      "Episode: 67 Replay Buffer 48977\n",
      "resetting\n",
      "Episode: 67, Reward: 0.00038472, Qmax: 0.00019309\n",
      "Episode: 68 Replay Buffer 49708\n",
      "resetting\n",
      "Episode: 68, Reward: 0.00033126, Qmax: 0.00019267\n",
      "Episode: 69 Replay Buffer 50439\n",
      "resetting\n",
      "Episode: 69, Reward: 0.00051028, Qmax: 0.00019188\n",
      "Episode: 70 Replay Buffer 51170\n",
      "resetting\n",
      "Episode: 70, Reward: 0.00031156, Qmax: 0.00019611\n",
      "Episode: 71 Replay Buffer 51901\n",
      "resetting\n",
      "Episode: 71, Reward: 0.00025278, Qmax: 0.00019229\n",
      "Episode: 72 Replay Buffer 52632\n",
      "resetting\n",
      "Episode: 72, Reward: 0.00039205, Qmax: 0.00019150\n",
      "Episode: 73 Replay Buffer 53363\n",
      "resetting\n",
      "Episode: 73, Reward: 0.00062244, Qmax: 0.00019446\n",
      "Episode: 74 Replay Buffer 54094\n",
      "resetting\n",
      "Episode: 74, Reward: 0.00052565, Qmax: 0.00019129\n",
      "Episode: 75 Replay Buffer 54825\n",
      "resetting\n",
      "Episode: 75, Reward: 0.00032281, Qmax: 0.00019326\n",
      "Episode: 76 Replay Buffer 55556\n",
      "resetting\n",
      "Episode: 76, Reward: 0.00037449, Qmax: 0.00018931\n",
      "Episode: 77 Replay Buffer 56287\n",
      "resetting\n",
      "Episode: 77, Reward: 0.00011830, Qmax: 0.00018368\n",
      "Episode: 78 Replay Buffer 57018\n",
      "resetting\n",
      "Episode: 78, Reward: 0.00096071, Qmax: 0.00018499\n",
      "Episode: 79 Replay Buffer 57749\n",
      "resetting\n",
      "Episode: 79, Reward: 0.00047907, Qmax: 0.00018584\n",
      "Episode: 80 Replay Buffer 58480\n",
      "resetting\n",
      "Episode: 80, Reward: 0.00047722, Qmax: 0.00018316\n",
      "Episode: 81 Replay Buffer 59211\n",
      "resetting\n",
      "Episode: 81, Reward: 0.00021996, Qmax: 0.00018060\n",
      "Episode: 82 Replay Buffer 59942\n",
      "resetting\n",
      "Episode: 82, Reward: 0.00003401, Qmax: 0.00018302\n",
      "Episode: 83 Replay Buffer 60673\n",
      "resetting\n",
      "Episode: 83, Reward: -0.00005361, Qmax: 0.00017953\n",
      "Episode: 84 Replay Buffer 61404\n",
      "resetting\n",
      "Episode: 84, Reward: 0.00020898, Qmax: 0.00018284\n",
      "Episode: 85 Replay Buffer 62135\n",
      "resetting\n",
      "Episode: 85, Reward: 0.00072697, Qmax: 0.00017946\n",
      "Episode: 86 Replay Buffer 62866\n",
      "resetting\n",
      "Episode: 86, Reward: 0.00028916, Qmax: 0.00018372\n",
      "Episode: 87 Replay Buffer 63597\n",
      "resetting\n",
      "Episode: 87, Reward: 0.00031881, Qmax: 0.00017790\n",
      "Episode: 88 Replay Buffer 64328\n",
      "resetting\n",
      "Episode: 88, Reward: 0.00076278, Qmax: 0.00017788\n",
      "Episode: 89 Replay Buffer 65059\n",
      "resetting\n",
      "Episode: 89, Reward: 0.00057964, Qmax: 0.00017630\n",
      "Episode: 90 Replay Buffer 65790\n",
      "resetting\n",
      "Episode: 90, Reward: 0.00047239, Qmax: 0.00017816\n",
      "Episode: 91 Replay Buffer 66521\n",
      "resetting\n",
      "Episode: 91, Reward: 0.00016906, Qmax: 0.00017575\n",
      "Episode: 92 Replay Buffer 67252\n",
      "resetting\n",
      "Episode: 92, Reward: 0.00029965, Qmax: 0.00017571\n",
      "Episode: 93 Replay Buffer 67983\n",
      "resetting\n",
      "Episode: 93, Reward: 0.00085483, Qmax: 0.00017407\n",
      "Episode: 94 Replay Buffer 68714\n",
      "resetting\n",
      "Episode: 94, Reward: 0.00100417, Qmax: 0.00017550\n",
      "Episode: 95 Replay Buffer 69445\n",
      "resetting\n",
      "Episode: 95, Reward: 0.00040016, Qmax: 0.00017639\n",
      "Episode: 96 Replay Buffer 70176\n",
      "resetting\n",
      "Episode: 96, Reward: 0.00049547, Qmax: 0.00017643\n",
      "Episode: 97 Replay Buffer 70907\n",
      "resetting\n",
      "Episode: 97, Reward: 0.00057806, Qmax: 0.00017429\n",
      "Episode: 98 Replay Buffer 71638\n",
      "resetting\n",
      "Episode: 98, Reward: 0.00044404, Qmax: 0.00017058\n",
      "Episode: 99 Replay Buffer 72369\n",
      "resetting\n",
      "Episode: 99, Reward: 0.00076261, Qmax: 0.00016587\n",
      "Episode: 100 Replay Buffer 73100\n",
      "resetting\n",
      "Episode: 100, Reward: 0.00051204, Qmax: 0.00016525\n",
      "Episode: 101 Replay Buffer 73831\n",
      "resetting\n",
      "Episode: 101, Reward: 0.00084686, Qmax: 0.00016863\n",
      "Episode: 102 Replay Buffer 74562\n",
      "resetting\n",
      "Episode: 102, Reward: 0.00041585, Qmax: 0.00016454\n",
      "Episode: 103 Replay Buffer 75293\n",
      "resetting\n",
      "Episode: 103, Reward: 0.00062373, Qmax: 0.00016461\n",
      "Episode: 104 Replay Buffer 76024\n",
      "resetting\n",
      "Episode: 104, Reward: 0.00016420, Qmax: 0.00016343\n",
      "Episode: 105 Replay Buffer 76755\n",
      "resetting\n",
      "Episode: 105, Reward: 0.00040814, Qmax: 0.00016476\n",
      "Episode: 106 Replay Buffer 77486\n",
      "resetting\n",
      "Episode: 106, Reward: 0.00089092, Qmax: 0.00016226\n",
      "Episode: 107 Replay Buffer 78217\n",
      "resetting\n",
      "Episode: 107, Reward: 0.00051259, Qmax: 0.00016018\n",
      "Episode: 108 Replay Buffer 78948\n",
      "resetting\n",
      "Episode: 108, Reward: 0.00086633, Qmax: 0.00016136\n",
      "Episode: 109 Replay Buffer 79679\n",
      "resetting\n",
      "Episode: 109, Reward: 0.00051703, Qmax: 0.00016246\n",
      "Episode: 110 Replay Buffer 80410\n",
      "resetting\n",
      "Episode: 110, Reward: 0.00031517, Qmax: 0.00016081\n",
      "Episode: 111 Replay Buffer 81141\n",
      "resetting\n",
      "Episode: 111, Reward: 0.00022784, Qmax: 0.00016283\n",
      "Episode: 112 Replay Buffer 81872\n",
      "resetting\n",
      "Episode: 112, Reward: 0.00008443, Qmax: 0.00015725\n",
      "Episode: 113 Replay Buffer 82603\n",
      "resetting\n",
      "Episode: 113, Reward: 0.00004056, Qmax: 0.00015672\n",
      "Episode: 114 Replay Buffer 83334\n",
      "resetting\n",
      "Episode: 114, Reward: 0.00038975, Qmax: 0.00015738\n",
      "Episode: 115 Replay Buffer 84065\n",
      "resetting\n",
      "Episode: 115, Reward: 0.00136957, Qmax: 0.00015465\n",
      "Episode: 116 Replay Buffer 84796\n",
      "resetting\n",
      "Episode: 116, Reward: 0.00061062, Qmax: 0.00015307\n",
      "Episode: 117 Replay Buffer 85527\n",
      "resetting\n",
      "Episode: 117, Reward: -0.00005277, Qmax: 0.00014753\n",
      "Episode: 118 Replay Buffer 86258\n",
      "resetting\n",
      "Episode: 118, Reward: 0.00028687, Qmax: 0.00014906\n",
      "Episode: 119 Replay Buffer 86989\n",
      "resetting\n",
      "Episode: 119, Reward: 0.00069894, Qmax: 0.00014652\n",
      "Episode: 120 Replay Buffer 87720\n",
      "resetting\n",
      "Episode: 120, Reward: 0.00058383, Qmax: 0.00014520\n",
      "Episode: 121 Replay Buffer 88451\n",
      "resetting\n",
      "Episode: 121, Reward: -0.00002227, Qmax: 0.00014283\n",
      "Episode: 122 Replay Buffer 89182\n",
      "resetting\n",
      "Episode: 122, Reward: 0.00093227, Qmax: 0.00014423\n",
      "Episode: 123 Replay Buffer 89913\n",
      "resetting\n",
      "Episode: 123, Reward: 0.00105341, Qmax: 0.00013890\n",
      "Episode: 124 Replay Buffer 90644\n",
      "resetting\n",
      "Episode: 124, Reward: 0.00076814, Qmax: 0.00014103\n",
      "Episode: 125 Replay Buffer 91375\n",
      "resetting\n",
      "Episode: 125, Reward: 0.00049014, Qmax: 0.00014015\n",
      "Episode: 126 Replay Buffer 92106\n",
      "resetting\n",
      "Episode: 126, Reward: 0.00074111, Qmax: 0.00013625\n",
      "Episode: 127 Replay Buffer 92837\n",
      "resetting\n",
      "Episode: 127, Reward: 0.00052455, Qmax: 0.00013181\n",
      "Episode: 128 Replay Buffer 93568\n",
      "resetting\n",
      "Episode: 128, Reward: 0.00059848, Qmax: 0.00013628\n",
      "Episode: 129 Replay Buffer 94299\n",
      "resetting\n",
      "Episode: 129, Reward: 0.00081431, Qmax: 0.00013565\n",
      "Episode: 130 Replay Buffer 95030\n",
      "resetting\n",
      "Episode: 130, Reward: 0.00015074, Qmax: 0.00013415\n",
      "Episode: 131 Replay Buffer 95761\n",
      "resetting\n",
      "Episode: 131, Reward: 0.00052630, Qmax: 0.00013632\n",
      "Episode: 132 Replay Buffer 96492\n",
      "resetting\n",
      "Episode: 132, Reward: 0.00056292, Qmax: 0.00013234\n",
      "Episode: 133 Replay Buffer 97223\n",
      "resetting\n",
      "Episode: 133, Reward: 0.00037745, Qmax: 0.00013120\n",
      "Episode: 134 Replay Buffer 97954\n",
      "resetting\n",
      "Episode: 134, Reward: 0.00093833, Qmax: 0.00013191\n",
      "Episode: 135 Replay Buffer 98685\n",
      "resetting\n",
      "Episode: 135, Reward: 0.00095970, Qmax: 0.00013454\n",
      "Episode: 136 Replay Buffer 99416\n",
      "resetting\n",
      "Episode: 136, Reward: -0.00023337, Qmax: 0.00013010\n",
      "Episode: 137 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 137, Reward: -0.00016014, Qmax: 0.00013175\n",
      "Episode: 138 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 138, Reward: 0.00028844, Qmax: 0.00013209\n",
      "Episode: 139 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 139, Reward: 0.00032331, Qmax: 0.00013179\n",
      "Episode: 140 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 140, Reward: 0.00071659, Qmax: 0.00013490\n",
      "Episode: 141 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 141, Reward: 0.00064054, Qmax: 0.00013128\n",
      "Episode: 142 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 142, Reward: 0.00043545, Qmax: 0.00013278\n",
      "Episode: 143 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 143, Reward: 0.00083531, Qmax: 0.00013248\n",
      "Episode: 144 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 144, Reward: 0.00081200, Qmax: 0.00013090\n",
      "Episode: 145 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 145, Reward: 0.00010426, Qmax: 0.00012724\n",
      "Episode: 146 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 146, Reward: -0.00009110, Qmax: 0.00013066\n",
      "Episode: 147 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 147, Reward: 0.00041411, Qmax: 0.00012983\n",
      "Episode: 148 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 148, Reward: 0.00070385, Qmax: 0.00012753\n",
      "Episode: 149 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 149, Reward: 0.00052958, Qmax: 0.00012766\n",
      "Episode: 150 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 150, Reward: 0.00045585, Qmax: 0.00012208\n",
      "Episode: 151 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 151, Reward: 0.00039270, Qmax: 0.00012550\n",
      "Episode: 152 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 152, Reward: 0.00044016, Qmax: 0.00012360\n",
      "Episode: 153 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 153, Reward: -0.00024252, Qmax: 0.00012726\n",
      "Episode: 154 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 154, Reward: 0.00008806, Qmax: 0.00012398\n",
      "Episode: 155 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 155, Reward: -0.00007333, Qmax: 0.00012614\n",
      "Episode: 156 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 156, Reward: 0.00046519, Qmax: 0.00012480\n",
      "Episode: 157 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 157, Reward: 0.00018222, Qmax: 0.00012267\n",
      "Episode: 158 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 158, Reward: 0.00070074, Qmax: 0.00012142\n",
      "Episode: 159 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 159, Reward: 0.00060539, Qmax: 0.00011982\n",
      "Episode: 160 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 160, Reward: -0.00007504, Qmax: 0.00012353\n",
      "Episode: 161 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 161, Reward: 0.00045947, Qmax: 0.00012010\n",
      "Episode: 162 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 162, Reward: 0.00121553, Qmax: 0.00012122\n",
      "Episode: 163 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 163, Reward: 0.00020939, Qmax: 0.00011584\n",
      "Episode: 164 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 164, Reward: 0.00022006, Qmax: 0.00011434\n",
      "Episode: 165 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 165, Reward: 0.00040100, Qmax: 0.00011164\n",
      "Episode: 166 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 166, Reward: 0.00013600, Qmax: 0.00011501\n",
      "Episode: 167 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 167, Reward: 0.00074990, Qmax: 0.00011687\n",
      "Episode: 168 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 168, Reward: -0.00012627, Qmax: 0.00011550\n",
      "Episode: 169 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 169, Reward: 0.00036085, Qmax: 0.00011204\n",
      "Episode: 170 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 170, Reward: 0.00047372, Qmax: 0.00011110\n",
      "Episode: 171 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 171, Reward: 0.00012943, Qmax: 0.00010960\n",
      "Episode: 172 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 172, Reward: 0.00042372, Qmax: 0.00010701\n",
      "Episode: 173 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 173, Reward: 0.00046465, Qmax: 0.00010709\n",
      "Episode: 174 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 174, Reward: 0.00076355, Qmax: 0.00010527\n",
      "Episode: 175 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 175, Reward: 0.00042614, Qmax: 0.00010359\n",
      "Episode: 176 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 176, Reward: 0.00030921, Qmax: 0.00010483\n",
      "Episode: 177 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 177, Reward: 0.00070632, Qmax: 0.00010296\n",
      "Episode: 178 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 178, Reward: 0.00029959, Qmax: 0.00010083\n",
      "Episode: 179 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 179, Reward: 0.00061389, Qmax: 0.00009592\n",
      "Episode: 180 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 180, Reward: 0.00088681, Qmax: 0.00010050\n",
      "Episode: 181 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 181, Reward: 0.00062572, Qmax: 0.00010056\n",
      "Episode: 182 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 182, Reward: 0.00043720, Qmax: 0.00010321\n",
      "Episode: 183 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 183, Reward: 0.00006032, Qmax: 0.00010472\n",
      "Episode: 184 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 184, Reward: -0.00007189, Qmax: 0.00010230\n",
      "Episode: 185 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 185, Reward: 0.00092539, Qmax: 0.00009873\n",
      "Episode: 186 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 186, Reward: 0.00080966, Qmax: 0.00009704\n",
      "Episode: 187 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 187, Reward: -0.00018138, Qmax: 0.00009684\n",
      "Episode: 188 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 188, Reward: 0.00031424, Qmax: 0.00009485\n",
      "Episode: 189 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 189, Reward: 0.00052408, Qmax: 0.00009367\n",
      "Episode: 190 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 190, Reward: 0.00087853, Qmax: 0.00009333\n",
      "Episode: 191 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 191, Reward: 0.00049934, Qmax: 0.00010036\n",
      "Episode: 192 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 192, Reward: 0.00117014, Qmax: 0.00009886\n",
      "Episode: 193 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 193, Reward: 0.00048514, Qmax: 0.00009583\n",
      "Episode: 194 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 194, Reward: 0.00034767, Qmax: 0.00009894\n",
      "Episode: 195 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 195, Reward: 0.00012228, Qmax: 0.00009320\n",
      "Episode: 196 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 196, Reward: 0.00056627, Qmax: 0.00010013\n",
      "Episode: 197 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 197, Reward: 0.00068625, Qmax: 0.00009923\n",
      "Episode: 198 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 198, Reward: 0.00066495, Qmax: 0.00010087\n",
      "Episode: 199 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 199, Reward: 0.00039765, Qmax: 0.00010022\n",
      "Episode: 200 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 200, Reward: 0.00056623, Qmax: 0.00009753\n",
      "Episode: 201 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 201, Reward: 0.00021928, Qmax: 0.00010295\n",
      "Episode: 202 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 202, Reward: 0.00025876, Qmax: 0.00009373\n",
      "Episode: 203 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 203, Reward: 0.00033760, Qmax: 0.00009978\n",
      "Episode: 204 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 204, Reward: 0.00096627, Qmax: 0.00009349\n",
      "Episode: 205 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 205, Reward: 0.00067686, Qmax: 0.00009324\n",
      "Episode: 206 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 206, Reward: 0.00082566, Qmax: 0.00009716\n",
      "Episode: 207 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 207, Reward: 0.00004038, Qmax: 0.00009502\n",
      "Episode: 208 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 208, Reward: 0.00035141, Qmax: 0.00009761\n",
      "Episode: 209 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 209, Reward: 0.00032865, Qmax: 0.00009669\n",
      "Episode: 210 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 210, Reward: 0.00029338, Qmax: 0.00009581\n",
      "Episode: 211 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 211, Reward: 0.00048704, Qmax: 0.00009727\n",
      "Episode: 212 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 212, Reward: 0.00076134, Qmax: 0.00009134\n",
      "Episode: 213 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 213, Reward: -0.00037247, Qmax: 0.00009494\n",
      "Episode: 214 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 214, Reward: 0.00034142, Qmax: 0.00009145\n",
      "Episode: 215 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 215, Reward: 0.00014112, Qmax: 0.00009576\n",
      "Episode: 216 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 216, Reward: 0.00034174, Qmax: 0.00009724\n",
      "Episode: 217 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 217, Reward: 0.00018937, Qmax: 0.00009262\n",
      "Episode: 218 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 218, Reward: 0.00085800, Qmax: 0.00009421\n",
      "Episode: 219 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 219, Reward: 0.00061228, Qmax: 0.00009027\n",
      "Episode: 220 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 220, Reward: 0.00071373, Qmax: 0.00008640\n",
      "Episode: 221 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 221, Reward: 0.00038728, Qmax: 0.00009208\n",
      "Episode: 222 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 222, Reward: 0.00015381, Qmax: 0.00009162\n",
      "Episode: 223 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 223, Reward: 0.00096739, Qmax: 0.00009022\n",
      "Episode: 224 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 224, Reward: 0.00047922, Qmax: 0.00008592\n",
      "Episode: 225 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 225, Reward: 0.00026354, Qmax: 0.00008611\n",
      "Episode: 226 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 226, Reward: 0.00007875, Qmax: 0.00008376\n",
      "Episode: 227 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 227, Reward: 0.00006472, Qmax: 0.00008840\n",
      "Episode: 228 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 228, Reward: 0.00071699, Qmax: 0.00008366\n",
      "Episode: 229 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 229, Reward: 0.00043480, Qmax: 0.00008730\n",
      "Episode: 230 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 230, Reward: 0.00008231, Qmax: 0.00008362\n",
      "Episode: 231 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 231, Reward: -0.00021246, Qmax: 0.00008019\n",
      "Episode: 232 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 232, Reward: 0.00013393, Qmax: 0.00008285\n",
      "Episode: 233 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 233, Reward: 0.00070475, Qmax: 0.00008162\n",
      "Episode: 234 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 234, Reward: 0.00008854, Qmax: 0.00007835\n",
      "Episode: 235 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 235, Reward: 0.00066079, Qmax: 0.00007992\n",
      "Episode: 236 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 236, Reward: 0.00006788, Qmax: 0.00007266\n",
      "Episode: 237 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 237, Reward: 0.00010961, Qmax: 0.00007638\n",
      "Episode: 238 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 238, Reward: -0.00011790, Qmax: 0.00007528\n",
      "Episode: 239 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 239, Reward: 0.00036320, Qmax: 0.00007544\n",
      "Episode: 240 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 240, Reward: 0.00029214, Qmax: 0.00007758\n",
      "Episode: 241 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 241, Reward: 0.00016845, Qmax: 0.00007351\n",
      "Episode: 242 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 242, Reward: 0.00089428, Qmax: 0.00007530\n",
      "Episode: 243 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 243, Reward: 0.00054437, Qmax: 0.00007586\n",
      "Episode: 244 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 244, Reward: 0.00098514, Qmax: 0.00007008\n",
      "Episode: 245 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 245, Reward: 0.00035961, Qmax: 0.00007618\n",
      "Episode: 246 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 246, Reward: 0.00066456, Qmax: 0.00007755\n",
      "Episode: 247 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 247, Reward: 0.00017061, Qmax: 0.00007888\n",
      "Episode: 248 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 248, Reward: 0.00014577, Qmax: 0.00008053\n",
      "Episode: 249 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 249, Reward: 0.00051664, Qmax: 0.00008278\n",
      "Episode: 250 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 250, Reward: 0.00015420, Qmax: 0.00008143\n",
      "Episode: 251 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 251, Reward: 0.00002676, Qmax: 0.00007545\n",
      "Episode: 252 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 252, Reward: 0.00076339, Qmax: 0.00007743\n",
      "Episode: 253 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 253, Reward: -0.00033894, Qmax: 0.00007504\n",
      "Episode: 254 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 254, Reward: 0.00051538, Qmax: 0.00007303\n",
      "Episode: 255 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 255, Reward: 0.00075728, Qmax: 0.00007256\n",
      "Episode: 256 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 256, Reward: 0.00050040, Qmax: 0.00006737\n",
      "Episode: 257 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 257, Reward: 0.00045941, Qmax: 0.00007327\n",
      "Episode: 258 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 258, Reward: 0.00066129, Qmax: 0.00006819\n",
      "Episode: 259 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 259, Reward: -0.00022952, Qmax: 0.00007096\n",
      "Episode: 260 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 260, Reward: 0.00001526, Qmax: 0.00007354\n",
      "Episode: 261 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 261, Reward: 0.00027706, Qmax: 0.00007567\n",
      "Episode: 262 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 262, Reward: -0.00000040, Qmax: 0.00007725\n",
      "Episode: 263 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 263, Reward: 0.00053150, Qmax: 0.00007226\n",
      "Episode: 264 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 264, Reward: 0.00059048, Qmax: 0.00007335\n",
      "Episode: 265 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 265, Reward: 0.00019721, Qmax: 0.00007211\n",
      "Episode: 266 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 266, Reward: 0.00005820, Qmax: 0.00007710\n",
      "Episode: 267 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 267, Reward: 0.00085914, Qmax: 0.00007382\n",
      "Episode: 268 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 268, Reward: -0.00001568, Qmax: 0.00007444\n",
      "Episode: 269 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 269, Reward: 0.00070515, Qmax: 0.00007002\n",
      "Episode: 270 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 270, Reward: -0.00011086, Qmax: 0.00007495\n",
      "Episode: 271 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 271, Reward: -0.00029147, Qmax: 0.00007244\n",
      "Episode: 272 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 272, Reward: 0.00052630, Qmax: 0.00007395\n",
      "Episode: 273 Replay Buffer 100000\n",
      "resetting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-c45d8f1d51d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mddpg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/School/Projects/summer2019/2/sum19/drl/src/model/ddpg/ddpg.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, save_every_episode, verbose, debug)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0ma_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# Update target networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/School/Projects/summer2019/2/sum19/drl/src/stock_trading.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, a_gradient)\u001b[0m\n\u001b[1;32m    129\u001b[0m         self.sess.run(self.optimize, feed_dict={\n\u001b[1;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_gradient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         })\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddpg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
