{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert generated (returns and cond nums) data to usable format to pass to env\n",
    "- check action bound: are actions in [-1,1]?\n",
    "- should we generate 1 long seqence or multiple? how does it effect convergence? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running from jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for compatible with python 3\n",
    "from __future__ import print_function\n",
    "import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "import numpy as np\n",
    "from utils.data import read_stock_history, index_to_date, date_to_index, normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.ddpg.actor import ActorNetwork\n",
    "from model.ddpg.critic import CriticNetwork\n",
    "from model.ddpg.ddpg import DDPG\n",
    "from model.ddpg.ornstein_uhlenbeck import OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "\n",
    "from stock_trading import StockActor, StockCritic, obs_normalizer, get_model_path, get_result_path, \\\n",
    "                          test_model, get_variable_scope, test_model_multiple, convert_R_output\n",
    "    \n",
    "from model.supervised.lstm import StockLSTM\n",
    "from model.supervised.cnn import StockCNN\n",
    "\n",
    "from environment.portfolio import PortfolioEnv, MultiActionPortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters calculated in R\n",
    "filepath = '/Users/zachariemartin/Desktop/School/Projects/summer2019/2/sum19/'\n",
    "\n",
    "alpha, beta, omega, Q_bar = convert_R_output(coef_path = filepath + 'coef.csv',\\\n",
    "                                                   Q_bar_path = filepath + 'Q_bar.csv', num_assets=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common settings\n",
    "batch_size = 64\n",
    "action_bound = 1.\n",
    "tau = 1e-3\n",
    "\n",
    "models = []\n",
    "model_names = []\n",
    "window_length_lst = [3]\n",
    "predictor_type_lst = ['cnn']\n",
    "use_batch_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nenv = PortfolioEnv(target_history, target_stocks)\\n\\nfor window_length in window_length_lst:\\n    for predictor_type in predictor_type_lst:\\n        name = 'DDPG_window_{}_predictor_{}'.format(window_length, predictor_type)\\n        model_names.append(name)\\n        tf.reset_default_graph()\\n        sess = tf.Session()\\n        tflearn.config.init_training_mode()\\n        action_dim = [nb_classes]\\n        state_dim = [nb_classes, window_length]\\n        variable_scope = get_variable_scope(window_length, predictor_type, use_batch_norm)\\n        with tf.variable_scope(variable_scope):\\n            actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, predictor_type, \\n                               use_batch_norm)\\n            critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\\n                                 learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(), \\n                                 predictor_type=predictor_type, use_batch_norm=use_batch_norm)\\n            actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\\n\\n            model_save_path = get_model_path(window_length, predictor_type, use_batch_norm)\\n            summary_path = get_result_path(window_length, predictor_type, use_batch_norm)\\n\\n            ddpg_model = DDPG(env, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\\n                              config_file='config/stock.json', model_save_path=model_save_path,\\n                              summary_path=summary_path)\\n            ddpg_model.initialize(load_weights=False, verbose=False)\\n            models.append(ddpg_model)\\n            \\nddpg_model.train()\\n\""
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# go from passing history in to env to generating within env\n",
    "\n",
    "'''\n",
    "env = PortfolioEnv(target_history, target_stocks)\n",
    "\n",
    "for window_length in window_length_lst:\n",
    "    for predictor_type in predictor_type_lst:\n",
    "        name = 'DDPG_window_{}_predictor_{}'.format(window_length, predictor_type)\n",
    "        model_names.append(name)\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        tflearn.config.init_training_mode()\n",
    "        action_dim = [nb_classes]\n",
    "        state_dim = [nb_classes, window_length]\n",
    "        variable_scope = get_variable_scope(window_length, predictor_type, use_batch_norm)\n",
    "        with tf.variable_scope(variable_scope):\n",
    "            actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, predictor_type, \n",
    "                               use_batch_norm)\n",
    "            critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\n",
    "                                 learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(), \n",
    "                                 predictor_type=predictor_type, use_batch_norm=use_batch_norm)\n",
    "            actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "            model_save_path = get_model_path(window_length, predictor_type, use_batch_norm)\n",
    "            summary_path = get_result_path(window_length, predictor_type, use_batch_norm)\n",
    "\n",
    "            ddpg_model = DDPG(env, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\n",
    "                              config_file='config/stock.json', model_save_path=model_save_path,\n",
    "                              summary_path=summary_path)\n",
    "            ddpg_model.initialize(load_weights=False, verbose=False)\n",
    "            models.append(ddpg_model)\n",
    "            \n",
    "ddpg_model.train()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data generated\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# # param dict\n",
    "# parameters = {}\n",
    "\n",
    "# # 10 assets\n",
    "num_assets = 10\n",
    "num_rows_per_asset = 5\n",
    "\n",
    "# # calculated in R\n",
    "# a = 0.007073296\n",
    "# b = 0.658588119\n",
    "\n",
    "# # import from R\n",
    "coef = pd.read_csv(filepath + 'coef.csv')\n",
    "\n",
    "# # mu, ar1, omega, alpha, beta 0,1,2,3,4 for each asset - 5 values for each asset so 500 total values\n",
    "# parameters['alpha'] = np.array([coef.loc[i,'x'] for i in range(3,(num_assets*num_rows_per_asset),5)])\n",
    "# parameters['beta'] = np.array([coef.loc[i,'x'] for i in range(4,(num_assets*num_rows_per_asset),5)])\n",
    "# parameters['omega'] = np.array([coef.loc[i,'x'] for i in range(2,(num_assets*num_rows_per_asset),5)])\n",
    "# parameters['Q_bar'] = pd.read_csv('Q_bar.csv').drop('Unnamed: 0',axis=1).to_numpy()\n",
    "# parameters['H_init'] = pd.read_csv('H_init.csv').drop('Unnamed: 0',axis=1).to_numpy()\n",
    "# parameters['Q'] = pd.read_csv('Q_init.csv').drop('Unnamed: 0',axis=1).to_numpy()\n",
    "# parameters['T'] = 1000\n",
    "# parameters['small_scalar'] = 1e-5\n",
    "# parameters['num_assets'] = 10\n",
    "\n",
    "parameters = {\n",
    "                'alpha' : np.array([coef.loc[i,'x'] for i in range(3,(num_assets*num_rows_per_asset),5)]),\n",
    "                'beta' : np.array([coef.loc[i,'x'] for i in range(4,(num_assets*num_rows_per_asset),5)]),\n",
    "                'omega' : np.array([coef.loc[i,'x'] for i in range(2,(num_assets*num_rows_per_asset),5)]),\n",
    "                'Q_bar' : pd.read_csv(filepath + 'Q_bar.csv').drop('Unnamed: 0',axis=1).to_numpy(),\n",
    "                'H_init' : pd.read_csv(filepath + 'H_init.csv').drop('Unnamed: 0',axis=1).to_numpy(),\n",
    "                'Q' : pd.read_csv(filepath + 'Q_init.csv').drop('Unnamed: 0',axis=1).to_numpy(),\n",
    "                'T' : 1000,\n",
    "                'small_scalar' : 1e-5,\n",
    "                'num_assets' : 10,\n",
    "                # calculated in R - parameters for Q process\n",
    "                'a' : 0.007073296,\n",
    "                'b' : 0.658588119,\n",
    "                'mean' : 100\n",
    "}\n",
    "\n",
    "env = PortfolioEnv(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = parameters['num_assets'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "After conv2d: (?, 11, 1, 32)\n",
      "Output: (?, 352)\n",
      "Build model from scratch\n"
     ]
    }
   ],
   "source": [
    "for window_length in window_length_lst:\n",
    "    for predictor_type in predictor_type_lst:\n",
    "        name = 'DDPG_window_{}_predictor_{}'.format(window_length, predictor_type)\n",
    "        model_names.append(name)\n",
    "        tf.reset_default_graph()\n",
    "        sess = tf.Session()\n",
    "        tflearn.config.init_training_mode()\n",
    "        action_dim = [nb_classes]\n",
    "        state_dim = [nb_classes, window_length]\n",
    "        variable_scope = get_variable_scope(window_length, predictor_type, use_batch_norm)\n",
    "        with tf.variable_scope(variable_scope):\n",
    "            actor = StockActor(sess, state_dim, action_dim, action_bound, 1e-4, tau, batch_size, predictor_type, \n",
    "                               use_batch_norm)\n",
    "            critic = StockCritic(sess=sess, state_dim=state_dim, action_dim=action_dim, tau=1e-3,\n",
    "                                 learning_rate=1e-3, num_actor_vars=actor.get_num_trainable_vars(), \n",
    "                                 predictor_type=predictor_type, use_batch_norm=use_batch_norm)\n",
    "            actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "            model_save_path = get_model_path(window_length, predictor_type, use_batch_norm)\n",
    "            summary_path = get_result_path(window_length, predictor_type, use_batch_norm)\n",
    "\n",
    "            ddpg_model = DDPG(env, sess, actor, critic, actor_noise, obs_normalizer=obs_normalizer,\n",
    "                              config_file='config/stock.json', model_save_path=model_save_path,\n",
    "                              summary_path=summary_path)\n",
    "            ddpg_model.initialize(load_weights=False, verbose=True)\n",
    "            models.append(ddpg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "Number of episodes:  500\n",
      "Episode: 0 Replay Buffer 0\n",
      "resetting\n",
      "Episode: 0, Reward: 0.00014532, Qmax: 0.00028298\n",
      "Episode: 1 Replay Buffer 731\n",
      "resetting\n",
      "Episode: 1, Reward: 0.00036220, Qmax: 0.00030536\n",
      "Episode: 2 Replay Buffer 1462\n",
      "resetting\n",
      "Episode: 2, Reward: 0.00008384, Qmax: 0.00030220\n",
      "Episode: 3 Replay Buffer 2193\n",
      "resetting\n",
      "Episode: 3, Reward: 0.00086087, Qmax: 0.00030040\n",
      "Episode: 4 Replay Buffer 2924\n",
      "resetting\n",
      "Episode: 4, Reward: 0.00078913, Qmax: 0.00029815\n",
      "Episode: 5 Replay Buffer 3655\n",
      "resetting\n",
      "Episode: 5, Reward: 0.00130539, Qmax: 0.00029476\n",
      "Episode: 6 Replay Buffer 4386\n",
      "resetting\n",
      "Episode: 6, Reward: 0.00079638, Qmax: 0.00029412\n",
      "Episode: 7 Replay Buffer 5117\n",
      "resetting\n",
      "Episode: 7, Reward: 0.00052881, Qmax: 0.00029290\n",
      "Episode: 8 Replay Buffer 5848\n",
      "resetting\n",
      "Episode: 8, Reward: 0.00048126, Qmax: 0.00029188\n",
      "Episode: 9 Replay Buffer 6579\n",
      "resetting\n",
      "Episode: 9, Reward: 0.00002979, Qmax: 0.00028807\n",
      "Episode: 10 Replay Buffer 7310\n",
      "resetting\n",
      "Episode: 10, Reward: -0.00001107, Qmax: 0.00028465\n",
      "Episode: 11 Replay Buffer 8041\n",
      "resetting\n",
      "Episode: 11, Reward: 0.00034454, Qmax: 0.00028267\n",
      "Episode: 12 Replay Buffer 8772\n",
      "resetting\n",
      "Episode: 12, Reward: -0.00005091, Qmax: 0.00027955\n",
      "Episode: 13 Replay Buffer 9503\n",
      "resetting\n",
      "Episode: 13, Reward: 0.00007914, Qmax: 0.00028102\n",
      "Episode: 14 Replay Buffer 10234\n",
      "resetting\n",
      "Episode: 14, Reward: 0.00033447, Qmax: 0.00027819\n",
      "Episode: 15 Replay Buffer 10965\n",
      "resetting\n",
      "Episode: 15, Reward: 0.00078647, Qmax: 0.00027500\n",
      "Episode: 16 Replay Buffer 11696\n",
      "resetting\n",
      "Episode: 16, Reward: 0.00059515, Qmax: 0.00027295\n",
      "Episode: 17 Replay Buffer 12427\n",
      "resetting\n",
      "Episode: 17, Reward: 0.00108176, Qmax: 0.00027299\n",
      "Episode: 18 Replay Buffer 13158\n",
      "resetting\n",
      "Episode: 18, Reward: 0.00128943, Qmax: 0.00027115\n",
      "Episode: 19 Replay Buffer 13889\n",
      "resetting\n",
      "Episode: 19, Reward: 0.00035301, Qmax: 0.00026642\n",
      "Episode: 20 Replay Buffer 14620\n",
      "resetting\n",
      "Episode: 20, Reward: 0.00007523, Qmax: 0.00026857\n",
      "Episode: 21 Replay Buffer 15351\n",
      "resetting\n",
      "Episode: 21, Reward: 0.00061335, Qmax: 0.00026284\n",
      "Episode: 22 Replay Buffer 16082\n",
      "resetting\n",
      "Episode: 22, Reward: 0.00067613, Qmax: 0.00026364\n",
      "Episode: 23 Replay Buffer 16813\n",
      "resetting\n",
      "Episode: 23, Reward: 0.00054601, Qmax: 0.00026304\n",
      "Episode: 24 Replay Buffer 17544\n",
      "resetting\n",
      "Episode: 24, Reward: 0.00057386, Qmax: 0.00026163\n",
      "Episode: 25 Replay Buffer 18275\n",
      "resetting\n",
      "Episode: 25, Reward: 0.00038439, Qmax: 0.00026194\n",
      "Episode: 26 Replay Buffer 19006\n",
      "resetting\n",
      "Episode: 26, Reward: 0.00032393, Qmax: 0.00026161\n",
      "Episode: 27 Replay Buffer 19737\n",
      "resetting\n",
      "Episode: 27, Reward: 0.00015552, Qmax: 0.00026295\n",
      "Episode: 28 Replay Buffer 20468\n",
      "resetting\n",
      "Episode: 28, Reward: 0.00045924, Qmax: 0.00025962\n",
      "Episode: 29 Replay Buffer 21199\n",
      "resetting\n",
      "Episode: 29, Reward: 0.00021222, Qmax: 0.00025754\n",
      "Episode: 30 Replay Buffer 21930\n",
      "resetting\n",
      "Episode: 30, Reward: -0.00003109, Qmax: 0.00025455\n",
      "Episode: 31 Replay Buffer 22661\n",
      "resetting\n",
      "Episode: 31, Reward: 0.00072689, Qmax: 0.00025130\n",
      "Episode: 32 Replay Buffer 23392\n",
      "resetting\n",
      "Episode: 32, Reward: -0.00001280, Qmax: 0.00025288\n",
      "Episode: 33 Replay Buffer 24123\n",
      "resetting\n",
      "Episode: 33, Reward: 0.00023815, Qmax: 0.00024869\n",
      "Episode: 34 Replay Buffer 24854\n",
      "resetting\n",
      "Episode: 34, Reward: 0.00013862, Qmax: 0.00024826\n",
      "Episode: 35 Replay Buffer 25585\n",
      "resetting\n",
      "Episode: 35, Reward: 0.00014318, Qmax: 0.00024574\n",
      "Episode: 36 Replay Buffer 26316\n",
      "resetting\n",
      "Episode: 36, Reward: 0.00034074, Qmax: 0.00024250\n",
      "Episode: 37 Replay Buffer 27047\n",
      "resetting\n",
      "Episode: 37, Reward: 0.00040966, Qmax: 0.00024446\n",
      "Episode: 38 Replay Buffer 27778\n",
      "resetting\n",
      "Episode: 38, Reward: 0.00048915, Qmax: 0.00024413\n",
      "Episode: 39 Replay Buffer 28509\n",
      "resetting\n",
      "Episode: 39, Reward: 0.00017064, Qmax: 0.00024444\n",
      "Episode: 40 Replay Buffer 29240\n",
      "resetting\n",
      "Episode: 40, Reward: -0.00011310, Qmax: 0.00023908\n",
      "Episode: 41 Replay Buffer 29971\n",
      "resetting\n",
      "Episode: 41, Reward: 0.00040635, Qmax: 0.00023880\n",
      "Episode: 42 Replay Buffer 30702\n",
      "resetting\n",
      "Episode: 42, Reward: 0.00083028, Qmax: 0.00024068\n",
      "Episode: 43 Replay Buffer 31433\n",
      "resetting\n",
      "Episode: 43, Reward: 0.00049203, Qmax: 0.00023649\n",
      "Episode: 44 Replay Buffer 32164\n",
      "resetting\n",
      "Episode: 44, Reward: 0.00039384, Qmax: 0.00023474\n",
      "Episode: 45 Replay Buffer 32895\n",
      "resetting\n",
      "Episode: 45, Reward: 0.00097387, Qmax: 0.00022772\n",
      "Episode: 46 Replay Buffer 33626\n",
      "resetting\n",
      "Episode: 46, Reward: 0.00077815, Qmax: 0.00022796\n",
      "Episode: 47 Replay Buffer 34357\n",
      "resetting\n",
      "Episode: 47, Reward: 0.00035468, Qmax: 0.00022854\n",
      "Episode: 48 Replay Buffer 35088\n",
      "resetting\n",
      "Episode: 48, Reward: 0.00010435, Qmax: 0.00022180\n",
      "Episode: 49 Replay Buffer 35819\n",
      "resetting\n",
      "Episode: 49, Reward: 0.00030331, Qmax: 0.00022014\n",
      "Episode: 50 Replay Buffer 36550\n",
      "resetting\n",
      "Episode: 50, Reward: 0.00002679, Qmax: 0.00021922\n",
      "Episode: 51 Replay Buffer 37281\n",
      "resetting\n",
      "Episode: 51, Reward: 0.00076599, Qmax: 0.00021609\n",
      "Episode: 52 Replay Buffer 38012\n",
      "resetting\n",
      "Episode: 52, Reward: 0.00014021, Qmax: 0.00021966\n",
      "Episode: 53 Replay Buffer 38743\n",
      "resetting\n",
      "Episode: 53, Reward: 0.00018239, Qmax: 0.00021067\n",
      "Episode: 54 Replay Buffer 39474\n",
      "resetting\n",
      "Episode: 54, Reward: 0.00036068, Qmax: 0.00021178\n",
      "Episode: 55 Replay Buffer 40205\n",
      "resetting\n",
      "Episode: 55, Reward: 0.00044869, Qmax: 0.00020770\n",
      "Episode: 56 Replay Buffer 40936\n",
      "resetting\n",
      "Episode: 56, Reward: 0.00087810, Qmax: 0.00020836\n",
      "Episode: 57 Replay Buffer 41667\n",
      "resetting\n",
      "Episode: 57, Reward: 0.00080779, Qmax: 0.00020895\n",
      "Episode: 58 Replay Buffer 42398\n",
      "resetting\n",
      "Episode: 58, Reward: 0.00048108, Qmax: 0.00020278\n",
      "Episode: 59 Replay Buffer 43129\n",
      "resetting\n",
      "Episode: 59, Reward: -0.00031865, Qmax: 0.00020019\n",
      "Episode: 60 Replay Buffer 43860\n",
      "resetting\n",
      "Episode: 60, Reward: 0.00027579, Qmax: 0.00020396\n",
      "Episode: 61 Replay Buffer 44591\n",
      "resetting\n",
      "Episode: 61, Reward: 0.00082375, Qmax: 0.00020297\n",
      "Episode: 62 Replay Buffer 45322\n",
      "resetting\n",
      "Episode: 62, Reward: 0.00057819, Qmax: 0.00020115\n",
      "Episode: 63 Replay Buffer 46053\n",
      "resetting\n",
      "Episode: 63, Reward: 0.00004124, Qmax: 0.00019871\n",
      "Episode: 64 Replay Buffer 46784\n",
      "resetting\n",
      "Episode: 64, Reward: 0.00045919, Qmax: 0.00019330\n",
      "Episode: 65 Replay Buffer 47515\n",
      "resetting\n",
      "Episode: 65, Reward: 0.00095214, Qmax: 0.00019948\n",
      "Episode: 66 Replay Buffer 48246\n",
      "resetting\n",
      "Episode: 66, Reward: 0.00069760, Qmax: 0.00019406\n",
      "Episode: 67 Replay Buffer 48977\n",
      "resetting\n",
      "Episode: 67, Reward: 0.00038472, Qmax: 0.00019309\n",
      "Episode: 68 Replay Buffer 49708\n",
      "resetting\n",
      "Episode: 68, Reward: 0.00033126, Qmax: 0.00019267\n",
      "Episode: 69 Replay Buffer 50439\n",
      "resetting\n",
      "Episode: 69, Reward: 0.00051028, Qmax: 0.00019188\n",
      "Episode: 70 Replay Buffer 51170\n",
      "resetting\n",
      "Episode: 70, Reward: 0.00031156, Qmax: 0.00019611\n",
      "Episode: 71 Replay Buffer 51901\n",
      "resetting\n",
      "Episode: 71, Reward: 0.00025278, Qmax: 0.00019229\n",
      "Episode: 72 Replay Buffer 52632\n",
      "resetting\n",
      "Episode: 72, Reward: 0.00039205, Qmax: 0.00019150\n",
      "Episode: 73 Replay Buffer 53363\n",
      "resetting\n",
      "Episode: 73, Reward: 0.00062244, Qmax: 0.00019446\n",
      "Episode: 74 Replay Buffer 54094\n",
      "resetting\n",
      "Episode: 74, Reward: 0.00052565, Qmax: 0.00019129\n",
      "Episode: 75 Replay Buffer 54825\n",
      "resetting\n",
      "Episode: 75, Reward: 0.00032281, Qmax: 0.00019326\n",
      "Episode: 76 Replay Buffer 55556\n",
      "resetting\n",
      "Episode: 76, Reward: 0.00037449, Qmax: 0.00018931\n",
      "Episode: 77 Replay Buffer 56287\n",
      "resetting\n",
      "Episode: 77, Reward: 0.00011830, Qmax: 0.00018368\n",
      "Episode: 78 Replay Buffer 57018\n",
      "resetting\n",
      "Episode: 78, Reward: 0.00096071, Qmax: 0.00018499\n",
      "Episode: 79 Replay Buffer 57749\n",
      "resetting\n",
      "Episode: 79, Reward: 0.00047907, Qmax: 0.00018584\n",
      "Episode: 80 Replay Buffer 58480\n",
      "resetting\n",
      "Episode: 80, Reward: 0.00047722, Qmax: 0.00018316\n",
      "Episode: 81 Replay Buffer 59211\n",
      "resetting\n",
      "Episode: 81, Reward: 0.00021996, Qmax: 0.00018060\n",
      "Episode: 82 Replay Buffer 59942\n",
      "resetting\n",
      "Episode: 82, Reward: 0.00003401, Qmax: 0.00018302\n",
      "Episode: 83 Replay Buffer 60673\n",
      "resetting\n",
      "Episode: 83, Reward: -0.00005361, Qmax: 0.00017953\n",
      "Episode: 84 Replay Buffer 61404\n",
      "resetting\n",
      "Episode: 84, Reward: 0.00020898, Qmax: 0.00018284\n",
      "Episode: 85 Replay Buffer 62135\n",
      "resetting\n",
      "Episode: 85, Reward: 0.00072697, Qmax: 0.00017946\n",
      "Episode: 86 Replay Buffer 62866\n",
      "resetting\n",
      "Episode: 86, Reward: 0.00028916, Qmax: 0.00018372\n",
      "Episode: 87 Replay Buffer 63597\n",
      "resetting\n",
      "Episode: 87, Reward: 0.00031881, Qmax: 0.00017790\n",
      "Episode: 88 Replay Buffer 64328\n",
      "resetting\n",
      "Episode: 88, Reward: 0.00076278, Qmax: 0.00017788\n",
      "Episode: 89 Replay Buffer 65059\n",
      "resetting\n",
      "Episode: 89, Reward: 0.00057964, Qmax: 0.00017630\n",
      "Episode: 90 Replay Buffer 65790\n",
      "resetting\n",
      "Episode: 90, Reward: 0.00047239, Qmax: 0.00017816\n",
      "Episode: 91 Replay Buffer 66521\n",
      "resetting\n",
      "Episode: 91, Reward: 0.00016906, Qmax: 0.00017575\n",
      "Episode: 92 Replay Buffer 67252\n",
      "resetting\n",
      "Episode: 92, Reward: 0.00029965, Qmax: 0.00017571\n",
      "Episode: 93 Replay Buffer 67983\n",
      "resetting\n",
      "Episode: 93, Reward: 0.00085483, Qmax: 0.00017407\n",
      "Episode: 94 Replay Buffer 68714\n",
      "resetting\n",
      "Episode: 94, Reward: 0.00100417, Qmax: 0.00017550\n",
      "Episode: 95 Replay Buffer 69445\n",
      "resetting\n",
      "Episode: 95, Reward: 0.00040016, Qmax: 0.00017639\n",
      "Episode: 96 Replay Buffer 70176\n",
      "resetting\n",
      "Episode: 96, Reward: 0.00049547, Qmax: 0.00017643\n",
      "Episode: 97 Replay Buffer 70907\n",
      "resetting\n",
      "Episode: 97, Reward: 0.00057806, Qmax: 0.00017429\n",
      "Episode: 98 Replay Buffer 71638\n",
      "resetting\n",
      "Episode: 98, Reward: 0.00044404, Qmax: 0.00017058\n",
      "Episode: 99 Replay Buffer 72369\n",
      "resetting\n",
      "Episode: 99, Reward: 0.00076261, Qmax: 0.00016587\n",
      "Episode: 100 Replay Buffer 73100\n",
      "resetting\n",
      "Episode: 100, Reward: 0.00051204, Qmax: 0.00016525\n",
      "Episode: 101 Replay Buffer 73831\n",
      "resetting\n",
      "Episode: 101, Reward: 0.00084686, Qmax: 0.00016863\n",
      "Episode: 102 Replay Buffer 74562\n",
      "resetting\n",
      "Episode: 102, Reward: 0.00041585, Qmax: 0.00016454\n",
      "Episode: 103 Replay Buffer 75293\n",
      "resetting\n",
      "Episode: 103, Reward: 0.00062373, Qmax: 0.00016461\n",
      "Episode: 104 Replay Buffer 76024\n",
      "resetting\n",
      "Episode: 104, Reward: 0.00016420, Qmax: 0.00016343\n",
      "Episode: 105 Replay Buffer 76755\n",
      "resetting\n",
      "Episode: 105, Reward: 0.00040814, Qmax: 0.00016476\n",
      "Episode: 106 Replay Buffer 77486\n",
      "resetting\n",
      "Episode: 106, Reward: 0.00089092, Qmax: 0.00016226\n",
      "Episode: 107 Replay Buffer 78217\n",
      "resetting\n",
      "Episode: 107, Reward: 0.00051259, Qmax: 0.00016018\n",
      "Episode: 108 Replay Buffer 78948\n",
      "resetting\n",
      "Episode: 108, Reward: 0.00086633, Qmax: 0.00016136\n",
      "Episode: 109 Replay Buffer 79679\n",
      "resetting\n",
      "Episode: 109, Reward: 0.00051703, Qmax: 0.00016246\n",
      "Episode: 110 Replay Buffer 80410\n",
      "resetting\n",
      "Episode: 110, Reward: 0.00031517, Qmax: 0.00016081\n",
      "Episode: 111 Replay Buffer 81141\n",
      "resetting\n",
      "Episode: 111, Reward: 0.00022784, Qmax: 0.00016283\n",
      "Episode: 112 Replay Buffer 81872\n",
      "resetting\n",
      "Episode: 112, Reward: 0.00008443, Qmax: 0.00015725\n",
      "Episode: 113 Replay Buffer 82603\n",
      "resetting\n",
      "Episode: 113, Reward: 0.00004056, Qmax: 0.00015672\n",
      "Episode: 114 Replay Buffer 83334\n",
      "resetting\n",
      "Episode: 114, Reward: 0.00038975, Qmax: 0.00015738\n",
      "Episode: 115 Replay Buffer 84065\n",
      "resetting\n",
      "Episode: 115, Reward: 0.00136957, Qmax: 0.00015465\n",
      "Episode: 116 Replay Buffer 84796\n",
      "resetting\n",
      "Episode: 116, Reward: 0.00061062, Qmax: 0.00015307\n",
      "Episode: 117 Replay Buffer 85527\n",
      "resetting\n",
      "Episode: 117, Reward: -0.00005277, Qmax: 0.00014753\n",
      "Episode: 118 Replay Buffer 86258\n",
      "resetting\n",
      "Episode: 118, Reward: 0.00028687, Qmax: 0.00014906\n",
      "Episode: 119 Replay Buffer 86989\n",
      "resetting\n",
      "Episode: 119, Reward: 0.00069894, Qmax: 0.00014652\n",
      "Episode: 120 Replay Buffer 87720\n",
      "resetting\n",
      "Episode: 120, Reward: 0.00058383, Qmax: 0.00014520\n",
      "Episode: 121 Replay Buffer 88451\n",
      "resetting\n",
      "Episode: 121, Reward: -0.00002227, Qmax: 0.00014283\n",
      "Episode: 122 Replay Buffer 89182\n",
      "resetting\n",
      "Episode: 122, Reward: 0.00093227, Qmax: 0.00014423\n",
      "Episode: 123 Replay Buffer 89913\n",
      "resetting\n",
      "Episode: 123, Reward: 0.00105341, Qmax: 0.00013890\n",
      "Episode: 124 Replay Buffer 90644\n",
      "resetting\n",
      "Episode: 124, Reward: 0.00076814, Qmax: 0.00014103\n",
      "Episode: 125 Replay Buffer 91375\n",
      "resetting\n",
      "Episode: 125, Reward: 0.00049014, Qmax: 0.00014015\n",
      "Episode: 126 Replay Buffer 92106\n",
      "resetting\n",
      "Episode: 126, Reward: 0.00074111, Qmax: 0.00013625\n",
      "Episode: 127 Replay Buffer 92837\n",
      "resetting\n",
      "Episode: 127, Reward: 0.00052455, Qmax: 0.00013181\n",
      "Episode: 128 Replay Buffer 93568\n",
      "resetting\n",
      "Episode: 128, Reward: 0.00059848, Qmax: 0.00013628\n",
      "Episode: 129 Replay Buffer 94299\n",
      "resetting\n",
      "Episode: 129, Reward: 0.00081431, Qmax: 0.00013565\n",
      "Episode: 130 Replay Buffer 95030\n",
      "resetting\n",
      "Episode: 130, Reward: 0.00015074, Qmax: 0.00013415\n",
      "Episode: 131 Replay Buffer 95761\n",
      "resetting\n",
      "Episode: 131, Reward: 0.00052630, Qmax: 0.00013632\n",
      "Episode: 132 Replay Buffer 96492\n",
      "resetting\n",
      "Episode: 132, Reward: 0.00056292, Qmax: 0.00013234\n",
      "Episode: 133 Replay Buffer 97223\n",
      "resetting\n",
      "Episode: 133, Reward: 0.00037745, Qmax: 0.00013120\n",
      "Episode: 134 Replay Buffer 97954\n",
      "resetting\n",
      "Episode: 134, Reward: 0.00093833, Qmax: 0.00013191\n",
      "Episode: 135 Replay Buffer 98685\n",
      "resetting\n",
      "Episode: 135, Reward: 0.00095970, Qmax: 0.00013454\n",
      "Episode: 136 Replay Buffer 99416\n",
      "resetting\n",
      "Episode: 136, Reward: -0.00023337, Qmax: 0.00013010\n",
      "Episode: 137 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 137, Reward: -0.00016014, Qmax: 0.00013175\n",
      "Episode: 138 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 138, Reward: 0.00028844, Qmax: 0.00013209\n",
      "Episode: 139 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 139, Reward: 0.00032331, Qmax: 0.00013179\n",
      "Episode: 140 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 140, Reward: 0.00071659, Qmax: 0.00013490\n",
      "Episode: 141 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 141, Reward: 0.00064054, Qmax: 0.00013128\n",
      "Episode: 142 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 142, Reward: 0.00043545, Qmax: 0.00013278\n",
      "Episode: 143 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 143, Reward: 0.00083531, Qmax: 0.00013248\n",
      "Episode: 144 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 144, Reward: 0.00081200, Qmax: 0.00013090\n",
      "Episode: 145 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 145, Reward: 0.00010426, Qmax: 0.00012724\n",
      "Episode: 146 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 146, Reward: -0.00009110, Qmax: 0.00013066\n",
      "Episode: 147 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 147, Reward: 0.00041411, Qmax: 0.00012983\n",
      "Episode: 148 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 148, Reward: 0.00070385, Qmax: 0.00012753\n",
      "Episode: 149 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 149, Reward: 0.00052958, Qmax: 0.00012766\n",
      "Episode: 150 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 150, Reward: 0.00045585, Qmax: 0.00012208\n",
      "Episode: 151 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 151, Reward: 0.00039270, Qmax: 0.00012550\n",
      "Episode: 152 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 152, Reward: 0.00044016, Qmax: 0.00012360\n",
      "Episode: 153 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 153, Reward: -0.00024252, Qmax: 0.00012726\n",
      "Episode: 154 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 154, Reward: 0.00008806, Qmax: 0.00012398\n",
      "Episode: 155 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 155, Reward: -0.00007333, Qmax: 0.00012614\n",
      "Episode: 156 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 156, Reward: 0.00046519, Qmax: 0.00012480\n",
      "Episode: 157 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 157, Reward: 0.00018222, Qmax: 0.00012267\n",
      "Episode: 158 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 158, Reward: 0.00070074, Qmax: 0.00012142\n",
      "Episode: 159 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 159, Reward: 0.00060539, Qmax: 0.00011982\n",
      "Episode: 160 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 160, Reward: -0.00007504, Qmax: 0.00012353\n",
      "Episode: 161 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 161, Reward: 0.00045947, Qmax: 0.00012010\n",
      "Episode: 162 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 162, Reward: 0.00121553, Qmax: 0.00012122\n",
      "Episode: 163 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 163, Reward: 0.00020939, Qmax: 0.00011584\n",
      "Episode: 164 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 164, Reward: 0.00022006, Qmax: 0.00011434\n",
      "Episode: 165 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 165, Reward: 0.00040100, Qmax: 0.00011164\n",
      "Episode: 166 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 166, Reward: 0.00013600, Qmax: 0.00011501\n",
      "Episode: 167 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 167, Reward: 0.00074990, Qmax: 0.00011687\n",
      "Episode: 168 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 168, Reward: -0.00012627, Qmax: 0.00011550\n",
      "Episode: 169 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 169, Reward: 0.00036085, Qmax: 0.00011204\n",
      "Episode: 170 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 170, Reward: 0.00047372, Qmax: 0.00011110\n",
      "Episode: 171 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 171, Reward: 0.00012943, Qmax: 0.00010960\n",
      "Episode: 172 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 172, Reward: 0.00042372, Qmax: 0.00010701\n",
      "Episode: 173 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 173, Reward: 0.00046465, Qmax: 0.00010709\n",
      "Episode: 174 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 174, Reward: 0.00076355, Qmax: 0.00010527\n",
      "Episode: 175 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 175, Reward: 0.00042614, Qmax: 0.00010359\n",
      "Episode: 176 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 176, Reward: 0.00030921, Qmax: 0.00010483\n",
      "Episode: 177 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 177, Reward: 0.00070632, Qmax: 0.00010296\n",
      "Episode: 178 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 178, Reward: 0.00029959, Qmax: 0.00010083\n",
      "Episode: 179 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 179, Reward: 0.00061389, Qmax: 0.00009592\n",
      "Episode: 180 Replay Buffer 100000\n",
      "resetting\n",
      "Episode: 180, Reward: 0.00088681, Qmax: 0.00010050\n",
      "Episode: 181 Replay Buffer 100000\n",
      "resetting\n"
     ]
    }
   ],
   "source": [
    "ddpg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
